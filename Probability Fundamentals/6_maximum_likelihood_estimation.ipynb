{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "918b7b0a-bc9b-43ec-b46c-02e507a682f6",
   "metadata": {},
   "source": [
    "# Maximum Likelihood Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3650b6-2fe4-4726-b115-2dda19605c09",
   "metadata": {},
   "source": [
    "Let's say I have some data, and I want to modify a function so that it fits to the data as closely as possible. Scratch that. I want to *maximize the likelihood* the function would generate the data I am observing. This is exactly what **maximum likelihood estimation** is, where we find the joint probability a given function would output some observed data. This technique is used to fit probability distributions to a given dataset, as well as train machine learning algorithms like linear regression and logistic regression. \n",
    "\n",
    "We will learn about maximum likelihood estimation by applying it first to a normal distribution. We will then extend that framework to fit a linear regression. Of course, a linear regression can be fit using least squares, but as we will learn maximum likelihood estimation will arrive at a nearly identical solution. Given that linear regression is the workhorse and building block of machine learning, this will tie ideas of how probability plays a role in machine learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3e8d03-0fbf-4810-9a79-06aef5583123",
   "metadata": {},
   "source": [
    "## Fitting a Normal Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae0a441-7ecc-4325-b63a-1eb8b9696651",
   "metadata": {},
   "source": [
    "Let's observe some measurements from a machining process at a shop, and plot it on a number line. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99aea1f3-8cb5-4e31-a801-02150aefa1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X = np.array([54.4,55.8,55.9,58.5,59.1,61.1,61.3,61.7,62.8,\n",
    "              63.2,63.5,63.6,63.6,63.7,64.0,64.2,65.0,65.0,\n",
    "              65.7,66.0,66.2,66.5,67.7,67.7,68.2,69.4,69.5,\n",
    "              70.2,70.5,71.8,72.8,72.8,73.8,76.2,77.4])\n",
    "\n",
    "plt.plot(X, [0 for _ in X], 'o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a06b10-264f-42d0-83cc-62353bc889fa",
   "metadata": {},
   "source": [
    "We suspect that this data follows a normal distribution and want to fit one to it. While we could simply take the mean and standard deviation of the data, and then use them in our parameters for a normal distribution, let's take a more probabilistic approach with maximum likelihood estimation. Other distributions like the exponential distribution would have to take that approach, and so would fitting a linear regression or logistic regression. \n",
    "\n",
    "We got a lot to break down. Let's first discussion the idea of joint likelihood for this purpose. Observe the datapoints and a given normal distribution below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa2364f-2ea4-4e67-86ea-7fdde8088b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X = np.array([54.4,55.8,55.9,58.5,59.1,61.1,61.3,61.7,62.8,\n",
    "              63.2,63.5,63.6,63.6,63.7,64.0,64.2,65.0,65.0,\n",
    "              65.7,66.0,66.2,66.5,67.7,67.7,68.2,69.4,69.5,\n",
    "              70.2,70.5,71.8,72.8,72.8,73.8,76.2,77.4])\n",
    "\n",
    "mu, sigma = 65.696, 5.469\n",
    "\n",
    "# plot the distribution and points\n",
    "x_range = np.arange(start=-3 * sigma + mu, stop=3 * sigma + mu, step=.01)\n",
    "plt.plot(X, [0 for _ in X], 'o')\n",
    "plt.plot(x_range, norm.pdf(x_range, mu, sigma))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd195b4-65d6-48d5-9931-3f21f2feb4d8",
   "metadata": {},
   "source": [
    "Imagine each of those points projecting themselves upward onto the curve, and the resulting y-values are the likelihoods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee78f631-a81f-4b44-a6e4-abc8d2feb188",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(x_range, norm.pdf(x_range, mu, sigma), color='orange')\n",
    "\n",
    "# plot lines\n",
    "for x in X:\n",
    "    plt.plot(np.array([x,x]),\n",
    "              np.array([0, norm.pdf(x, mu, sigma)]),\n",
    "             'bo', linestyle=\"--\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ac18ad-b24f-43fd-867e-32e841e2c71d",
   "metadata": {},
   "source": [
    "Those resulting y-values resembling the likelihoods are what we want to multiply together, called the **joint likelihood**. We can calculate this using the `prod()` function on a NumPy array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf32e552-1a30-4f4e-9b90-0cdcf354e7f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "X = np.array([54.4,55.8,55.9,58.5,59.1,61.1,61.3,61.7,62.8,\n",
    "              63.2,63.5,63.6,63.6,63.7,64.0,64.2,65.0,65.0,\n",
    "              65.7,66.0,66.2,66.5,67.7,67.7,68.2,69.4,69.5,\n",
    "              70.2,70.5,71.8,72.8,72.8,73.8,76.2,77.4])\n",
    "\n",
    "mu, sigma = 65.696, 5.469\n",
    "\n",
    "likelihoods = norm.pdf(X, mu, sigma)\n",
    "joint_likelihood = norm.pdf(X, mu, sigma).prod()\n",
    "\n",
    "print(f\"Likelihoods: {likelihoods}\")\n",
    "print(f\"\\nJoint Likelihood: {joint_likelihood}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfee689f-4dca-47f2-a903-74791f7c5825",
   "metadata": {},
   "source": [
    "This joint likelihood is likely to get very small as shown above, as we are multiplying a lot of probabilities together. Behind the scenes, summing log-likelihoods (instead of multiplication) are used to avoid floating point underflows but we will let NumPy do that work. The point is we want to adjust mu and sigma so we maximize that total joint likelihood (hence maximum likelihood estimation). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1777f5-8517-4a3a-8fba-182a87ae814a",
   "metadata": {},
   "source": [
    "To apply maximum likelihood estimation, we need to learn an optimization algorithm first. Normally we would use a technique like gradient descent or stochastic gradient descent. However, to prevent the need for a crash course in Calculus (another Anaconda course on that later!) we will use a simpler algorithm called hill climbing. **Hill climbing** allows us to use a random-based, but methodical, search that makes random adjustments to values but only accepts values that improve towards an objective. In this case, the objective is the maximum total joint likelihood. \n",
    "\n",
    "Recall a normal distribution accepts parameters *mu* $ \\mu $ and *sigma* $ \\sigma $. We will repeatedly make random adjustments to these two parameters and only accept adjustments that improve the total maximum likelhood. But what are the random adjustments going to be? We do not necessarily want to brute-force with uniformly random values, but we can sample from a standard normal distribution (with a mean of 0 and standard deviation of 1) so we can get mostly small adjustments near 0 but occasional large adjustments in the tail. Observe the standard normal distribution below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf225b79-3f58-4c59-866c-65389b2ec30c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "\n",
    "x_range = np.arange(start=-3, stop=3, step=.01)\n",
    "plt.plot(x_range, norm.pdf(x_range, loc=0, scale=1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143720ce-eb34-4dcc-8194-67e6aef5eb4a",
   "metadata": {},
   "source": [
    "It may seem pretty meta (and conceptually circuituous) that we use a normal distribution to randomly adjust another normal distribution. But it works! Remember that one is the normal distribution we are fitting to our data, and another normal distribution generates random adjustments to $ \\mu $ and $ \\sigma $. \n",
    "\n",
    "Let's put these concepts together and use hill climbing to execute maximum likelihood estimation. We will start $ \\mu $ at one of the data points and the standard deviation at 1. They just roughly have to start somewhere near the points and then hill climbing will adjust them accordingly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed96d03-f7a0-4239-ae7d-f0529fe73d9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# declare X datapoints \n",
    "X = np.array([54.4,55.8,55.9,58.5,59.1,61.1,61.3,61.7,62.8,\n",
    "              63.2,63.5,63.6,63.6,63.7,64.0,64.2,65.0,65.0,\n",
    "              65.7,66.0,66.2,66.5,67.7,67.7,68.2,69.4,69.5,\n",
    "              70.2,70.5,71.8,72.8,72.8,73.8,76.2,77.4])\n",
    "\n",
    "# start mean at the first data point, and sigma at 1\n",
    "# does not matter where they start, they should just be \n",
    "# somewhere inside or close to the dataset \n",
    "mu, sigma = X[0], 1\n",
    "\n",
    "# generates a random value from a standard normal distribution\n",
    "# where mean is 0, and standard deviation is 1 \n",
    "def random_adj(): return np.random.normal(loc=0, scale=1, size=1)[0]\n",
    "\n",
    "# start best joint likelihood at 0\n",
    "best_joint_likelihood = 0\n",
    "\n",
    "# do 10,000 random adjustments to mu and sigma \n",
    "for i in range(10_000):\n",
    "\n",
    "    # randomly adjust mu and sigma \n",
    "    mu_adj, sigma_adj = random_adj(), random_adj()\n",
    "\n",
    "    mu += mu_adj\n",
    "    sigma += sigma_adj\n",
    "\n",
    "    # calculate the new joint likelihood of all data points \n",
    "    new_joint_likelihood = norm.pdf(X, mu, sigma).prod()\n",
    "\n",
    "    # if joint likelihood improves, keep the changes to mu and sigma \n",
    "    if new_joint_likelihood > best_joint_likelihood:\n",
    "        best_joint_likelihood = new_joint_likelihood\n",
    "        print(f\"mu,sigma -> {mu}, {sigma}\")\n",
    "    else:\n",
    "        # otherwise undo changes \n",
    "        mu -= mu_adj\n",
    "        sigma -= sigma_adj\n",
    "\n",
    "# plot the result \n",
    "x_range = np.arange(start=-3 * sigma + mu, stop=3 * sigma + mu, step=.01)\n",
    "\n",
    "plt.plot(X, [0 for _ in X], 'o')\n",
    "plt.plot(x_range, norm.pdf(x_range, mu, sigma))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779d8c4a-9e03-4fb2-a60f-60778fbd7067",
   "metadata": {},
   "source": [
    "Based on the print outputs and the plot above, you can see the curve moved effectively to fit to the points. Notice too that if we take the mean and standard devation of the data, it matches what the maximum likelihood estimation found. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370355c9-af9a-4921-81fd-c86e5a1e62eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X.mean(), X.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b753e200-fa75-4596-a9f7-e8d9907aa109",
   "metadata": {},
   "source": [
    "This was not just a needless academic exercise though. You can fit any probability distribution given data using this technique. \n",
    "\n",
    "Now let's extend this idea to fitting a linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992f744c-e7ef-435d-bfe4-90c81dd88a3c",
   "metadata": {},
   "source": [
    "## Using MLE to Fit a Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ee18a5-793d-41b5-83f2-c04176a6bbaf",
   "metadata": {},
   "source": [
    "Next let's use maximum likelihood estimation to fit a linear regression, which is fitting a straight line through some points. Observe the data below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333ade71-6a7a-40f8-9ddb-d0ab5926b6f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X = np.array([2.9,9.5,5.1,0.9,2.0,2.3,5.2,7.7,7.9,4.1,9.7,6.3,4.9,6.2,4.2,\n",
    "              3.1,3.9,8.7,1.2,9.6,0.8,3.0,5.6,7.3,3.7,3.5,2.6,5.0,1.7,9.1,1.9,2.4,0.3,5.7,9.0])\n",
    "\n",
    "Y = np.array([7.58,18.83,8.71,0.6,6.06,0.64,12.09,13.65,15.34,8.6,16.32,11.78,9.78,8.44,9.18,\n",
    "              3.04,7.65,10.96,1.47,17.52,2.21,4.76,13.03,12.29,10.2,7.88,3.01,8.92,2.23,15.08,\n",
    "              5.42,5.53,-0.5,11.66,14.7])\n",
    "\n",
    "\n",
    "plt.plot(X, Y, 'o') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a81faf-1b10-409c-9af5-f3543d708cb0",
   "metadata": {},
   "source": [
    "We want to fit a line through these points for the purposes of analyzing/predicting the relationship between two variables $ X $ and $ Y $. Recall the formula for a linear function. \n",
    "\n",
    "$ \n",
    "y = mx + b\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18774257-b639-4175-ab39-c6abea882321",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_range = np.arange(start=X.min(), stop=X.max(), step=.01)\n",
    "plt.plot(X, Y, 'o') \n",
    "plt.plot(x_range, 1.71160239*x_range + 0.53778288) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2590f557-ff58-487b-9287-8b25b734ade3",
   "metadata": {},
   "source": [
    "Imagine that a normal distribution is following along that line, where the mean is going to trace along that line but the standard deviation is going to stay constant. We can treat the actual y-values as the input variable (what typically is *x* going into the PDF) and the predicted y-values as the mean. This means we need to solve for the slope *m*, the intercept *b*, and the standard deviation $ \\sigma $. Because randomly adjusting three coefficients at once is a lot of movement, we will only randomly select one of them to adjust on each iteration. \n",
    "\n",
    "Other than these few changes, we are really fitting a normal distribution using maximum likelihood estimation just like before! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6e5734-7cb8-40a0-9512-ff06fe1eb7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# declare X and Y datapoints\n",
    "X = np.array([2.9,9.5,5.1,0.9,2.0,2.3,5.2,7.7,7.9,4.1,9.7,6.3,4.9,6.2,4.2,\n",
    "              3.1,3.9,8.7,1.2,9.6,0.8,3.0,5.6,7.3,3.7,3.5,2.6,5.0,1.7,9.1,1.9,2.4,0.3,5.7,9.0])\n",
    "\n",
    "Y = np.array([7.58,18.83,8.71,0.6,6.06,0.64,12.09,13.65,15.34,8.6,16.32,11.78,9.78,8.44,9.18,\n",
    "              3.04,7.65,10.96,1.47,17.52,2.21,4.76,13.03,12.29,10.2,7.88,3.01,8.92,2.23,15.08,\n",
    "              5.42,5.53,-0.5,11.66,14.7])\n",
    "\n",
    "m, b, sigma = 1,1,1\n",
    "\n",
    "# generates a random value from a standard normal distribution\n",
    "# where mean is 0, and standard deviation is 1\n",
    "def random_adj(): return np.random.normal(loc=0, scale=1, size=1)[0]\n",
    "\n",
    "# start best joint likelihood at 0\n",
    "best_joint_likelihood = 0\n",
    "\n",
    "# do 20,000 random adjustments to mu and sigma\n",
    "for i in range(30_000):\n",
    "\n",
    "    # randomly adjust mu and sigma\n",
    "    rand_adj = random_adj()\n",
    "    rand_coeff = np.random.randint(0,3)\n",
    "    if rand_coeff == 0:\n",
    "        m += rand_adj\n",
    "    elif rand_coeff == 1:\n",
    "        b += rand_adj\n",
    "    elif rand_coeff == 2:\n",
    "        sigma += rand_adj\n",
    "\n",
    "    # calculate the new joint likelihood of all data points\n",
    "    new_joint_likelihood = np.array([norm.pdf(y, m*x+b, sigma) for x,y in zip(X,Y)]).prod()\n",
    "\n",
    "    # if joint likelihood improves, keep the changes to mu and sigma\n",
    "    if new_joint_likelihood > best_joint_likelihood:\n",
    "        best_joint_likelihood = new_joint_likelihood\n",
    "        print(f\"m,b,sigma -> {m}, {b}, {sigma}\")\n",
    "    else:\n",
    "        # otherwise undo changes\n",
    "        if rand_coeff == 0:\n",
    "            m -= rand_adj\n",
    "        elif rand_coeff == 1:\n",
    "            b -= rand_adj\n",
    "        elif rand_coeff == 2:\n",
    "            sigma -= rand_adj\n",
    "\n",
    "# plot the result\n",
    "x_range = np.arange(start=X.min(), stop=X.max(), step=.01)\n",
    "\n",
    "plt.plot(X, Y, 'o')\n",
    "plt.plot(x_range, m*x_range+b)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8daac7f2-46c4-49a7-a1dd-f0ca06440bc1",
   "metadata": {},
   "source": [
    "If we compare to a convetional least squares method, we get a nearly identical result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e013915-a4db-4737-835e-31e6702ff096",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lr = LinearRegression().fit(X.reshape([-1, 1]), Y)\n",
    "print(f\"m: {lr.coef_[0]}\")                            \n",
    "print(f\"b: {lr.intercept_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11294ca6-d4fa-4447-a1cd-ff8680549b98",
   "metadata": {},
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e1cf17-556e-4049-a217-dcf9e851e652",
   "metadata": {},
   "source": [
    "A bar is experiencing a traffic slowdown on Wednesdays and the manager is thinking of adding a happy hour promotion. \n",
    "\n",
    "![img](data:image/svg+xml;base64,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)\n",
    "\n",
    "He wants to understand how many customer arrivals there are each hour on average. He asks the bartender to record 12 customer arrivals and how much time lapsed between each one (in hours). Complete the code below (replacing the question marks \"?\") to perform maximum likelihood estimation and find the `lambda` parameter (the mean number of customers per hour) on the exponential distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778e6b25-a2e1-4e9e-bce7-e9eb039ee53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import expon\n",
    "\n",
    "# observed times between each customer \n",
    "X = np.array([0.27922493, 0.44124056, 0.50967118, 0.44413533, 0.67243048, 0.01870771,\n",
    "     0.08661839, 0.29967495, 1.68386979, 0.30475119, 0.65567402, 0.0098742\n",
    "])\n",
    "\n",
    "# solve for mean time between each customer, start at 1 \n",
    "lambda_rate = 1\n",
    "\n",
    "# start best likelihood \n",
    "best_likelihood = 0\n",
    "\n",
    "# perform hill climbing and adjust lambda rate\n",
    "# to improve joint likelihood\n",
    "for i in range(?):\n",
    "     random_adj = np.random.normal(loc=?,scale=?, size=1)[0]\n",
    "     lambda_rate += ?\n",
    "\n",
    "     new_likelihood = expon.pdf(?, scale = 1 / lambda_rate).prod()\n",
    "     if ? < ?:\n",
    "          best_likelihood = new_likelihood\n",
    "     else:\n",
    "          lambda_rate -= ?\n",
    "\n",
    "print(lambda_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f19a85c-747c-406c-b871-46c85306231e",
   "metadata": {},
   "source": [
    "### SCROLL DOWN FOR ANSWER\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "v "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878c7b08-b8fd-4912-83ca-8c52bbad8d09",
   "metadata": {},
   "source": [
    "You should roughly find there is about 2.21 customers per hour given these intervals. Use hill climbing and maximum likelihood estimation in the same manner we did for the normal distribution to solve for that lambda $ \\lambda $ parameter.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe8eddf-f475-4364-86d3-ca50b5b0141b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import expon\n",
    "\n",
    "# observed times between each customer\n",
    "X = np.array([0.27922493, 0.44124056, 0.50967118, 0.44413533, 0.67243048, 0.01870771,\n",
    "     0.08661839, 0.29967495, 1.68386979, 0.30475119, 0.65567402, 0.0098742\n",
    "])\n",
    "\n",
    "# solve for mean time between each customer, start at 1 \n",
    "lambda_rate = 1\n",
    "\n",
    "# start best likelihood \n",
    "best_likelihood = 0\n",
    "\n",
    "# perform hill climbing and adjust lambda rate\n",
    "# to improve joint likelihood\n",
    "for i in range(10_000):\n",
    "     random_adj = np.random.normal(loc=0,scale=1, size=1)[0]\n",
    "     lambda_rate += random_adj\n",
    "\n",
    "     new_likelihood = expon.pdf(X, scale = 1 / lambda_rate).prod()\n",
    "     if best_likelihood < new_likelihood:\n",
    "          best_likelihood = new_likelihood\n",
    "     else:\n",
    "          lambda_rate -= random_adj\n",
    "\n",
    "print(lambda_rate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda-panel-2023.05-py310",
   "language": "python",
   "name": "conda-env-anaconda-panel-2023.05-py310-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
